\chapter{Architecture}
\label{chap:architecture}

In the previous chapter the reasoning and simplification procedures
used by STRUCTure were explained. In this chapter we will discuss
how these procedures are used by our solver.  A top view of the
architecture is shown in Figure \ref{fig:architecture}. There are
four main components out of which only \emph{Searching Backtrack} is
essential.  The other three are optimizations to improve performance
on larger industrial applications. In the order of relevance the
components are:

\begin{enumerate}
  \item \emph{Searching Backtrack}: performs a distributed solution search
  and clause learning (clauses are not added here);
  \item \emph{Restart}: starts search, stops search and extends
  formula with clauses learned in Searching Backtrack.
  \item \emph{Simplification}: performs simplifications on formula (possible
  augmented with learned clauses);
  \item \emph{Circuit Preprocessing}: reverts and simplifies 
  part of the original Boolean circuit.
\end{enumerate}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{dia/architecture}
  \caption{STRUCTure has four components. \emph{Searching}
  is mandatory, the other three are optimizations
  for industrial class instances.}
  \label{fig:architecture}
\end{figure}


\section{Constellation}
\label{sec:constellation}

Constellation is a distributed programming framework developed at
VU University Amsterdam \cite{mine:constellation} as part of the
Ibis project (\url{http://www.cs.vu.nl/ibis/}).  In Constellation
the unit of work is an \emph{activity}. Activities can perform some work,
spawn new activities, send messages to other activities and process
incoming messages (see Figure \ref{fig:constellation}).

Constellation does not provide any shared memory mechanism,
but activities running on the same node are free to use the
common address space. Broadcasting or cancellation is not
available. Unfortunately, they are needed in STRUCTure (see
Section \ref{sssec:blackhole}) to stop the search.

Under the hood, Constellation works by running a fixed number of
worker threads on each node which execute activities.  Workers
maintain a queue of events (such as new activities or messages
passed) which are processed in last-in-first-out (LIFO) order.
When a thread runs out of work it attempts to steal more jobs first
from other workers on the same node, and then from other workers
on different nodes.  Local steals are performed in LIFO order,
while remote steals are performed in FIFO (first-in-first-out)
order. Remote steals are slower than local steals because they
involve network communication and they are performed only at
regular intervals. \footnote{Stealing orders are fixed by STRUCTure.
Constellation allows changing the order.}

\begin{figure}
  \centering
  \includegraphics[width=0.45\linewidth]{dia/constellation}
  \caption{In Constellation activities spawn new
  activities and communicate by message passing.
  Activities can be executed on any node (represented by
  distinct colors) in the distributed process.}
  \label{fig:constellation}
\end{figure}


To fit the Constellation model, in STRUCTure activities are
independent of each other (except for passed messages). Consequently,
these activities needs to build some internal data structure for
the reasoning procedures they perform. However, most data structures
can be reused by an activity's children if they are executed on the
same machine. From profiling we have seen that this overhead can
account for 10\% to 40\% of the total running time, not including
the time garbage collector spends freeing old structures.

\section{Activities}

In STRUCTure, an activity's goal is to find a satisfying assignment
for a single instance, prove unsatisfiability or learn something new
about the instance. Some activities can spawn new activities to
solve smaller instances.

Activities return to their parents a solution to instances which can be
\textsf{SATISFIABLE}, \textsf{UNSATISFIABLE} or \textsf{UNKNOWN}. If
a satisfying assignment was found the solution includes units,
otherwise it includes learned clauses from conflicts.

\begin{figure}
  \centering
  \includegraphics[width=1.1\linewidth]{dia/activity}
  \caption{Activities in STRUCTure. Rectangles represent activities,
  arrows show dependence among activities. Learning is
  performed in the fourth component, against the arrows direction.}
  \label{fig:activities}
\end{figure}


\subsection{Circuit Preprocessing}

The first step in STRUCTure is \emph{Circuit Preprocessing}. Many
industrial applications encode logic circuits as CNF formulas.
All logic gates, except XOR, require a number of clauses linear in the number
of inputs and are handled easily by most solvers. On the other hand,
XOR gates require an exponential number of clauses and many solvers
have difficulty handling them.

This part of the solver attempts to reverse and simplify part of the
original circuit using XOR gates extraction and dependent variable
removal (DVR) described in Sections \ref{ssec:xor-extraction}
and \ref{ssec:dvr}.  Unfortunately, many reasoning procedures do
not handle XOR gates well, so, after DVR, the remaining gates are
turned back into CNF clauses. To avoid generating big clauses in
our implementation DVR ignores variables that, if removed,
would generate XOR gates with a large number of inputs.


\subsection{Simplification}

After Circuit Preprocessing, \emph{Simplification} component applies
different simplification procedures to strengthen the clauses and
reduce the formula. It consists of several activities, shown in
Figure \ref{fig:activities}.

First, \emph{Blocked clauses elimination} activity performs
blocked clause elimination to remove redundant clauses (see Section
\ref{ssec:bce}). Then, \emph{Variable elimination} activity performs
variable elimination to remove redundant variables (see Section
\ref{ssec:ve}). \emph{Failed Literal Probing} learns new units
and binaries (see section below).  Finally, the \emph{Simplify}
activity is similar to the Solve activity from the searching
backtrack component (see Section \ref{sssec:solve}).

Except for the Failed Literals Probing activity, the Simplification
component is sequential. However, in practice, the computation
performed by the Searching Backtrack component takes the most part
of the solver's running time.


\subsubsection{Failed Literals Probing (FLP)}
\label{sssec:flp}

\emph{Failed Literals Probing} is a preprocessing step with
the goal of discovering new units, binaries and equivalent literals
\cite{Lynce03probing-basedpreprocessing}.  It is similar to
performing a small search (see Section \ref{ssec:searching-tree}) from
the root until depth 1 each time choosing a different variable to
branch first.

\begin{myprop}[Failed literals]
  If the propagation, $BCP(F, u)$ of a literal $u$ is inconsistent
  then any satisfying assignment of $F$ must contain the negation
  of that literal. $\neg u$ is called \emph{failed literal}.
  $\emptyset \in BCP(F, u) \Rightarrow F \equiv F \cup \{\neg u\}$.
\end{myprop}


\emph{Failed Literal Probing} activity chooses a fixed number of
variables (by default 128) and sends them to \emph{Propagate}
activity to perform a weaker version of boolean constraint
propagation of both polarities $u$ and $\neg u$. Variables are
selected using the same variable selection heuristic used for
branching (see Section \ref{ssec:branching}).

If $u, u_0, u_1, \ldots$ are propagated literals and $v, v_0, v_1,
\ldots \in UP(F, u)$ then Propagate activity learns the clauses in
Table \ref{tbl:propagate-learn}. There are plenty of other binary
clauses to be learned (i.e. one for each literal $\in UP(F, u)$),
but many of them are redundant and do not improve performance if
added to formula (e.g. FLP can also rediscover the binaries in
initial formula).

\begin{table}[h]
  \centering
  \framebox{
    \begin{tabular}{c c c}
      initial & propagated & learned \\
      \hline
      \hline
      $u$ & $\emptyset$ & $\neg u$ \\
      \hline
      $u$ & $v$ & $v$ \\
      $\neg u$ & $v$ & \\
      \hline
      $u$ & $v$ & $u \leftrightarrow v$ \\
      $\neg u$ & $\neg v$ & \\
      \hline
      $u$ & $u_0$ & $u \rightarrow u_0$ \\
      $u_0$ & &
    \end{tabular}
  }
  \caption{Learned units and binaries by Propagate activity.}
  \label{tbl:propagate-learn}
\end{table}


\subsection{Restarting}
\label{ssec:restarting}

After Simplification, the \emph{Restart}
component performs starting and stopping of the distributed
search. Restarting is an idea coined by Gnomes, Selman and Kautz
in \cite{Gomes:1998:BCS:295240.295710} who observed that the
running time of SAT solvers varies a lot depending on the variables
branched first.

Restarting has two roles. First, if the search chooses a variables
order that does not lead to fast solution then the search is
restarted as a different order of variables might be better.  Second,
when the distributed search is stopped, the Restart activity extends
the formula with the learned clauses. Then, the augmented formula
is simplified by the Simplification component.

Each searching period is called a \emph{generation} and, currently,
the time to live of a generation is fixed to 10 second plus 10
seconds for each new generation.  Section \ref{sssec:blackhole}
describes how the distributed computation is stopped using the
BlackHole activities.


\subsection{Searching Backtrack}
\label{ssec:searching-tree}

The last component of STRUCTure is the \emph{Searching backtrack}
component which searches for a solution.  After some time the search
is stopped by the Restart component, discussed in the previous
section. When the search finishes a solution was found or a
set of learned clauses is returned. The component consists of several
activities discussed in the next sections: \emph{Split, Select,
Branch, BlackHole} and \emph{Solve}. Only Select, Branch and Solve
are essential for a functional solver.

At the top of search tree there are not enough activities to occupy
all participating workers, but as more activities are spawned and
distributed more workers join the search.

In Constellation work stealing is used to distribute work among
threads.  Local steals are performed in LIFO order, which means that
smaller jobs (more variables assigned) are stolen first.  On the
other hand, remote steals are performed in FIFO order, corresponding to
larger jobs (less variables assigned) being stolen first.  Adding more
nodes makes the search go more in breadth, while adding more CPUs
per node makes the search go more in depth.


\subsubsection{Split}
\label{sssec:split}

The first step in finding a solution is to try to divide a formula
into several smaller independent formulas.  The formula $F$ in
the example below is a conjunction of two independent formulas,
$F_1$ and $F_2$. $F$ is consistent only if both $F_1$ and $F_2$
are consistent. If $F_1$ or $F_2$ are inconsistent, then $F$
is inconsistent.

\begin{align}
  F &= (1 \lor 2 \lor 3) \land (1 \lor \neg 4) \land (5 \lor \neg 6) \land (6 \lor \neg 7) \\
  F_1 &= (1 \lor 2 \lor 3) \land (1 \lor \neg 4) \\
  F_2 &= (5 \lor \neg 6) \land (6 \lor \neg 7) \\
  F &= F_1 \land F_2
\end{align}

\begin{myprop}
  If $F(u_1, \ldots, u_{k_1}, \ldots, u_{k_2}, \ldots, u_{k_n}) =
  F_1(u_1, \ldots, u_{k_1}) \land \ldots \land F_n(u_{k_{n-1}+1},
  \ldots, u_{k_n})$ for $1 < k_1 < \ldots <k_n$ then $F$
  is consistent whenever all instances $F_1, \ldots, F_n$
  are consistent.
\end{myprop}

\emph{Split} provides an opportunity for parallelization. The
resulted independent formulas are evaluated separately.


\subsubsection{Select}
\label{ssec:branching}

\emph{Select} activity performs variable selection which is
an essential part of the basic DPLL algorithm (see Section
\ref{ssec:dpll}).  It is preferable that both polarities of the
branching variable reduce the formula as much as possible after
propagation.  If, for example, $\{ u \}$ is an unit clause then
choosing $u$ for branching is pointless because branch $\neg u$
will return a contradiction, while branch $u$ will not reduce the
formula further.

Heule suggests in \cite{mine:march} to do a forward
backtracking step on a fixed number of variables (i.e. look-ahead)
in order to:
\begin{inparaenum}[a)]
  \item learn new units and binaries and
  \item estimate size of formula after branching.
\end{inparaenum} The branching variable is picked such that
the formula is simplified as much as possible.

In STRUCTure performing look-ahead at every branch gives worse
performance. We have found that look-ahead is more expensive
than simply assigning a score based on the clauses containing
that variable. Moreover, the additional reasoning procedures,
especially hyper-unit resolution (see Section \ref{ssec:hbr}),
offset the benefits of learning new units.

The scoring heuristic is described below. To determine the
best variable for branching, a measure $H_u$ is computed for
every variable.  $H_u$ was inspired by a similar measure in
\cite{mine:oksolver}. The variable with the highest score is
selected for branching.  $H_u$ is calculated based on measures of
both variable's polarities: $S_u$ and $S_{\neg u}$. $T_u$ is how much
a formula is reduced if literal $u$ is assigned without looking
at propagations ($u \rightarrow v$). $S_u$ improves on $T_u$ by
including scores of implied units.

\begin{align}
  T_u &= \sum_{u \in C \in F}{2^{-|C|}} + \sum_{u \in C \in F, |C| = 2}{2^{-4}} \\
  S_u &= T_u + \sum_{\{ u \rightarrow v \} \in F}{T_v}
\end{align}

$T_u$ favors literals that appear in many short clauses.  $S_u$
favors literals for which implications appear in many short clauses.
Finally $H_u$ is computed using the following function:

\begin{align}
  H_u &= G(S_u, S_{\neg u}) \\
  G(p, n) &= \sum_{i=1}^{n}{(F_i(p, n) + F_i(n, p))} \\
  F_i(p, n) &= a_i \cdot p^{b_i} \cdot n^{c_i}, a_i, b_i, c_i \in \mathbb{Z} \\
  G, F_i &: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}, i = 1 \ldots n
\end{align}

$G$ is a generalization of a similar formula used in other
look-ahead solvers such as \cite{mine:march}: $G'(p, n) =
1024 \times p \cdot n + p + n$. $G$ must have a couple of properties:
\begin{inparaenum}[a)]
  \item it must be easy (and fast) to compute (e.g. no exponentiation or logarithm); and
  \item it must be symmetrical.
\end{inparaenum}

For $n = 3$ and $a_i, b_i, c_i \in \mathbb{Z}_5$ best results were
obtained for polynomial:
\begin{align}
  G(p, n) &=  p^2 \cdot n^2 \cdot (2 \cdot p^2 \cdot n^2 + 2 \cdot p \cdot n + p + n)
\end{align}
Increasing ranges for $n, a_i, b_i, c_i$ did not result in
significant better performance.


\subsubsection{Branch}

After Select, \emph{Branch} activity uses the branching variable,
$b$, to generate two instances $F_+ = F \cup \{b\}$ and $F_- \cup
\{\neg b\}$ and attempts to solve both of them.

\begin{myprop}
  If $F$ is a CNF formula and $u$ is a branching variable
  $F$ is consistent if and only if $F_+$ or $F_-$ is consistent.
\end{myprop}

If a solution is found, the Branch activity forwards it to
its parent activity before the remaining branch (if any) is
completed. Constellation cannot cancel activities, but local threads
perform events in LIFO order so the solution is propagated fast to
the Restart activity which will stop the computation.


\subsubsection{BlackHole}
\label{sssec:blackhole}

To end the search, the Restart component (see Section
\ref{ssec:restarting}) requires \emph{BlackHole} activities which, when
signaled, kills instances by immediately returning \textsf{UNKNOWN}
instead of solving them.

\emph{BlackHole} hits the limits of Constellation's distributed
model because, in order to stop current computation, all instances of
BlackHole activity on all nodes must know that the computation has
ended, but activities know little about nodes executing them. In
a shared memory model this is a matter of setting a flag, but, with
different address spaces, the Restart activity must broadcast its
intent to stop the distributed computation.

In STRUCTure broadcast is implemented using a centralized model as
in Figure \ref{fig:broadcast}.  All instances are accompanied by an
identifier of an unique \emph{TracerMaster} activity responsible for
broadcasting stop notifications. On every node a \emph{TracerSlave}
activity is responsible for receiving the notifications. When a
node executes its first activity TracerSlave registers itself to
TracerMaster which maintains a list of all available slaves. At the
end of the current generation the Register activity contacts TracerMaster
which in turn contacts all registered slaves.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{dia/broadcast}
  \caption{Broadcast is implemented with
  TracerSlave on each node and a single TracerMaster}
  \label{fig:broadcast}
\end{figure}


\subsubsection{Solve}
\label{sssec:solve}

\emph{Solve} activities attempt to find a satisfying assignment for
associated formulas by performing verification and simplification
steps of the DPLL algorithm (see Section \ref{ssec:dpll}).
In STRUCTure the simplification step is composed of the following
reasoning techniques:
\begin{itemize}
  \item Boolean constraint propagation (\ref{ssec:bcp});
  \item Hyper Unit Resolution (\ref{ssec:hbr});
  \item Equivalent Literal Renaming (\ref{ssec:eqlr});
  \item Hidden Tautology Elimination (\ref{ssec:hte});
  \item Subsumming and Self-Subsumming (\ref{ssec:sss});
  \item Pure Literal Rule (\ref{ssec:pl}).
\end{itemize}

This activity can be extended to run other solvers. Currently,
if the formula is in $2$-SAT, then the Solve activity performs the
linear time algorithm from \cite{mine:tarjan}.

If a Solve activity finds a solution for its formula (i.e.  formula
is determined to be consistent or inconsistent), then the solution is
propagated back up on the search tree. If the formula is inconsistent,
then it is called that a \emph{conflict} has been reached with current
variable assignments.  However, if the satisfiability of formula
cannot be determined, then the activity generates a smaller formula,
the \emph{core}, resulted from simplification step. The solution of
the core is used to reconstruct a solution of the original formula.


\subsection{Clause Learning}
\label{ssec:learning}

\emph{Clause learning} is a technique for augmenting the search
that first appeared in GRASP solver \cite{Marques-silva99grasp:a}
and now is part of almost all modern sat-solvers.

During search when a conflict is encountered it means that the
current variable assignment cannot lead to a solution of the original
problem. Because at least one of the assigned literals must be false
the current assignment is transformed into a disjunction of negation
of assigned literals.

Take for example the searching tree in Figure \ref{fig:learning}.
Assignment of literals $1, 2, \neg 4$ leads to a conflict, so at
least one of the variables must have a different value. Hence,
the clause $\neg 1 \lor \neg 2 \lor 4$ is deduced. In a future
generation if 2 out of the 3 variables involved in the conflict are
assigned the same value then the remaining variable is constrained
to have a different value.

\begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{dia/learning}
  \caption{Conflicts are transformed into clauses by forbidding
  same variable assignment.
  $\overline{(1 \land 2 \land \neg 4)} \equiv (\neg 1 \lor \neg 2 \lor 4)$}
  \label{fig:learning}
\end{figure}

Now consider two conflicts $C_1 = \{ \neg 1, \neg 2, 4 \}$ and
$C_2 = \{ \neg 1, 2 \}$.  By applying the resolution operator
(see Section \ref{ssec:resolution}) clause $C_3 = \{ \neg 1,
4 \}$ is obtained which is a subset of $C_1$; therefore $C_1$ can
be substituted for $C_3$ strengthening the learned clause. Similarly,
conflicts $C_4 = \{ 1, 3, 5 \}$ and $C_5 = \{1, 3, \neg 5\}$ are
merged into $C_6 = \{ 1, 3 \}$.

\begin{myprop}
  If assignment of literals $u_1, \ldots u_i$, $i \ge 0$, leads to
  a conflict then the \emph{conflict clause} $\{ \neg u_1, \ldots
  \neg u_i \}$ must be satisfied by any satisfiable assignment of
  the original formula.
\end{myprop}

\begin{myprop}
  If $C_1 = \{ v_1, \ldots, v_i, \mathbf{u} \}$ and $C_2 = \{
  v_1, \ldots, v_i, \mathbf{\neg u}, v_{i+1}, \ldots v_{i+j} \}$
  are two conflict clauses, $i, j \ge 0$, then $C = \{ v_1, v_2,
  \ldots, v_{i+j} \}$ must also be satisfied.
\end{myprop}

Sharing learned clauses between threads in a parallel/distributed
environment is tricky.  Some solvers replicate learned clauses that
meet some criteria \cite{Hamadi09manysat:a}, others keep an unique
database of all learned clauses \cite{mine:miraxt}.  In STRUCTure
clause learning is performed when a conflict is reached, but the
search can use the new clauses only after they are appended to
the problem by the Restart activity.  The key observation is that
a clause learned on a branch is already satisfied by the variable
assignment pertaining to a different branch of the searching tree.

In Figure \ref{fig:activities} each activity from the
searching component will return a set of learned clauses to its
parent. When a generation is stopped Restart activity extends
the formula with the learned clauses received from the search
component.  Learned clauses are often analyzed and strengthened
\cite{Audemard_ageneralized}. However, STRUCTure does not have
an analyzing component, but a similar effect is achieved by
Simplification component when it receives the augmented formula.

STRUCTure also learns some implications resulted from reasoning
procedures applied at the top nodes of the searching tree. This
improves some cases where the initial formula is so large, that
simplification procedures take up the entire time allocated for
current generation.
