\chapter{Reasonings}

\section {Branching}

The goal of branching is to chooses a variable which will be
assigned with both $True$ and $False$. It is preferable that both
polarities reduce the formula as much as possible after propagating
that polarity. If for example, $\{ u \}$ is an unit clause then
choosing $u$ for branching is pointless because branch $\neg u$
will return a contradiction, while branch $u$ will not reduce the
formula further.

March (\cite{mine:march}) suggests a doing a look-ahead \todo{explain
look ahead} on a fixed number of variables before choosing a
branching variable.  The goal is to
\begin{inparaenum}[\itshape a \upshape)]
\item learn new units and binaries (simplifying the formula) and
\item estimate the size of formulas after branching.
\end{inparaenum}

My tests (see figure \ref{fig:branching}) indicate that performing
look-ahead at every branch gives worse performance. My intuition
is that look-ahead is very expensive compared to simply assigning
a score based on clauses in which a variable is present. Moreover,
unlike other look-ahead solvers, in STRUCTure hyper-\{unit/binary\}
resolution simulates part of look-ahead on all variables (see
section \ref{sec:hyper-binary-resolution}).

To determine the best variable for branching a score $S_u$ is
computed for each literal (one literal for each poliarity of
every variable). $T_u$ is how much a formula is reduced if $u$
is assigned without looking at propagations ($u \rightarrow v$).
$S_u$ improves on $T_u$ by including scores of implied units.

\begin{align}
  T_u &= \sum_{u \in C \in F}{2^{-|C|}} + \sum_{u \in C \in F, |C| = 2}{2^{-4}} \\
  S_u &= T_u + \sum_{\{ u \rightarrow v \} \in F}{T_v}
\end{align}

The variable score, $H_u$ is computed based on both polarities.
The variable with the highest score is selected for branching. Note that
$H_u$ is simetrical on $u$ and $\neg u$.

\begin{align}
  G &: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R} \\
  \label{eqn:variable-score}
  G(p, n) &=  p^2 \cdot n^2 \cdot (2 \cdot p^2 \cdot n^2 + 2 \cdot p \cdot n + p + n); \\
  H_u &= G(S_u, S_{\neg u})
\end{align}


The formula $G_u$ is not magic, but a generalizatoin of the formula
used in other look-ahead solvers (such as \cite{mine:march}): $G'(p,
n) = 1024 \times p \cdot n + p + n$. Some properties $G$ must have:
\begin{inparaenum}[\itshape a \upshape)]
\item G must be easy (and fast) to compute (e.g. no exponentiation, or
logarith). and
\item G must be symetrical.
\end{inparaenum} I chose $G$ to be a polynomial of form:

\begin{align}
  G, F_i &: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}, i = 1 \ldots n \\
  G(p, n) &= \sum_{i=1}^{n}{(F_i(p, n) + F_i(n, p))} \\
  F_i(p, n) &= a_i \cdot p^{b_i} \cdot n^{c_i}, a_i, b_i, c_i \in \mathbb{Z}
\end{align}

I optimized for $n = 3$ and $a_i, b_i, c_i \in \mathbb{Z}_5$ by evaluating
the solver on instances described in annex \ref{app:easy-instances}
to obtain the formula \ref{eqn:variable-score}. Increasing ranges for
$n, a_i, b_i, c_i$ did not result in significant better performance.


\footnote{XXX: An example of this kind of heuristics is propz by Li
[17]. In short, these heuristics work as follows: Every variable
that satisfies certain constraints on its occurrences in binary
clauses will enter the lookahead procedure.}


\begin{figure}[h]
  \centering
  \framebox{
    \includegraphics[width=0.75\textwidth]{data/branching.eps}
  }
  \caption{Branching with look ahead}
  \label{fig:branching}
\end{figure}

\section{Basics}

\begin{myprop}[Necessary assignment]
  $(u \rightsquigarrow v) \land (u \rightsquigarrow \neg v) \Rightarrow v$
\end{myprop}

\begin{myprop}[Contradiction]
  $u \rightsquigarrow \neg u \Rightarrow \neg u$
\end{myprop}


\section{Unit Propapagation}

\todo{Add citation}

\begin{mydef}
  Given a formula $F$ and a literal $u$ \emph{unit propagation}
  of $u$ in $F$, denoted $UP(F, u)$, simplifies F according to two rules:
  \begin{enumerate}
    \item $\neg u$ is removed from every clause containing it
    \item every clause containing literal $u$ is removed
  \end{enumerate}
\end{mydef}

\begin{mydef}
  $UP(F)$ is the formula resulted from repeating the process of unit propagation
  of all unit clauses in a formula $F$ until a fixed point.
\end{mydef}

Example:

$\begin{array}{rl}
  F & = u_2 \land (u_0 \lor u_1 \lor u_2) \land (\neg u_0 \lor u_1) \land (\neg u_1 \lor \neg u_2) \\
  UP(F, u_0) & = u_2 \land u_1 \land (\neg u_1 \lor \neg u_2) \\
  UP(F) & = u_2 \land \neg u_1 \\
\end{array}$


\begin{myprop}
  $F \equiv UP(F)$. $F$ and its unit propagation $UP(F)$ are equivalent.
\end{myprop}

\todo{Text first}
\begin{myprop}[Contradictory units]
  $\{u\}, \{\neg u\} \in F \Rightarrow \emptyset \in UP(F)$.
\end{myprop}

\begin{myprop}
  $\emptyset \in UP(F) \Rightarrow F \equiv F \cup \emptyset$. $F$ is a
  contradiction if $UP(F)$ is a contradiction.
\end{myprop}

\begin{myprop}
  $\emptyset \in UP(F, u) \Rightarrow F \equiv F \cup \{\neg u\}$. If propagation
  of a literal is inconsistent then any satisfying assignment of $F$
  must contain the negation of that literal.
\end{myprop}

\begin{myprop}
  $\{ v \} \in UP(F, u) \Rightarrow F \equiv F \cup \{\neg u, v\}$. Under $F$
  implication $u \rightarrow v$ is true.
\end{myprop}

\emph{Propagate} activity (section \ref{ssec:searching-tree}) tries to exploit
the previous propositions to reduce the complexity of a formula.
Given variable $u$ Propagate activity performs unit propagations $UP(F, u)$
and $UP(F, \neg u)$. Variables to be tested are chosen by Lookahead
activity. In our implementation Propagate is doing 32 simultaneously
propagations. \todo{Should I describe implementation of Propagate? Where?}

If $u, u_0, u_1, \ldots$ are propagated literals, $v, v_0, v_1,
\ldots \in UP(F, u)$ then Propagate activity learns the clauses in table
\ref{tbl:propagate-learn}. There are plenty of other binary clauses
to be learned (one for each literal $\in UP(F, u)$, but my testing
showed that most of them are redundant and don't improve performance.

\begin{table}[h]
  \centering
  \framebox{
    \begin{tabular}{c c c}
      initial & propagated & learned \\
      \hline
      \hline
      $u$ & $\emptyset$ & $\neg u$ \\
      \hline
      $u$ & $v$ & $v$ \\
      $\neg u$ & $v$ & \\
      \hline
      $u$ & $v$ & $u \leftrightarrow v$ \\
      $\neg u$ & $\neg v$ & \\
      \hline
      $u$ & $u_0$ & $u \rightarrow u_0$ \\
      $u_0$ & &
    \end{tabular}
  }
  \caption{Learned units and binaries by Propagate activity}
  \label{tbl:propagate-learn}
\end{table}


\section{Equivalent Literals Renaming}

From proposition \ref{myprop:equivalent-literals-in-if} if $u \rightsquigarrow v$
and $v \rightsquigarrow u$ then $u$ and $v$ are equivalent.
Consider the implication graph: $u \rightsquigarrow v$ and $v \rightsquigarrow u$
if and only if $u$ and $v$ are in the same strongly connected component.

\begin{figure}[h]
  \centering
  \framebox{
    \includegraphics[width=0.58\textwidth]{data/equivalent.pdf}
  }
  \caption{2, 3, 4, 5 are equivalent literals because they are in the same
  strongly connected components.}
  \label{fig:equivalent}
\end{figure}

\begin{myprop}
  If $u \leftrightsquigarrow v$ then every occurrence of $v$ can be replaced
  with $v$. More generally, in the implication graph each strongly
  connected component can be collapsed to a single literal and, in
  the formula every literal from the component can be replaced with
  the new literal.
\end{myprop}


\section{Hidden Literal Addition}

\emph{Hidden Literal Addition} ($HLA(F, C)$) was introduced in
\cite{Heule:2010:CEP:1928380.1928406} as a technique to enlarge a clause $C$
in a formula $F$ with new literals $L$, such that $F \equiv F \setminus \{C\} \
\cup \{ C \cup L \}$. By itself this enlargement is not useful, but it
is used by other simplification algorithms to improve results.

\begin{mydef}
  $HLA(F, C)$ is the unique clause resulting from repeating the following
  clause extension steps until a fixed point: if there is a literal $u_0 \in C$
  and a literal $v$ such that $\neg v \rightsquigarrow u \text{ in } \I_F$
  extend $C$ with $\neg v$, $C := C \cup \{ \neg v \}$.
\end{mydef}

\begin{myprop}
  $C \subseteq HLA(F, C)$.
\end{myprop}

\begin{figure}[h]
  \centering
  \framebox{
    \includegraphics[width=0.48\textwidth]{data/hla.pdf}
  }
  \caption{Hidden Literal Addition: Example of implication graph}
  \label{fig:hla}
\end{figure}

To find extension of a clause, $C$, we can perform a search in the implication
graph, $G_F$, starting from negation of literals in clause. $HLA(F, C)$ will
be the set of negations of the visited literals.

For example let's suppose we have the clause $C = \{ 2, \neg 4, 3\}$
and the implicatin graph in figure \ref{fig:hla}. Then $HLA(F, C)
= \{2, \neg 4, \mathbf{\neg 5, \neg 6}, 3, \mathbf{\neg 2} \}$.


\section{Tautology Elimination (TE)}

\begin{myprop}[TE]
  \label{myprop:removal-of-tautologies}
  A formula $F$ is equivalent with $F$ with all tautologies removed.
  $F \equiv F \setminus \{ C \in F | C \text{ is tautology}\}$.
\end{myprop}


\section{Hidden Tautology Elimination (HTE)}
\label{sec:hte}

\begin{myprop}[HTE]
  $HLA(F, C) \text{ is a tautology} \Rightarrow F \equiv F \setminus \{C\}$.
\end{myprop}

\begin{proof}
  Following from proposition \ref{myprop:removal-of-tautologies},
  if $HLA(F, C)$ is a tautology it can be removed from formula.
  $F \equiv F \setminus \{C\} \cup \{HLA(F, C)\} \equiv F \setminus \{C\}$
\end{proof}

\todo{Classify reasonings}
\begin{myprop}[HTE $\supseteq$ TE]
  Hidden Tautology Elimination is at least as good as Tautology Elimination.
\end{myprop}

\begin{proof}
  If $C$ is a tautology then $HLA(F, C) \supseteq C$ is also a tautology,
  therefore HTE can remove at least all tautologies.
\end{proof}

\begin{myprop}[HTE and equivalent literals]
  $u \leftrightsquigarrow \neg v \text{ and } \{ u, v \} \subset C \Rightarrow HLA(F,
  C) \text{ is tautology}$
\end{myprop}

In the previous example $HLA(F, C) = \{2, \neg 4, \mathbf{\neg 5,
\neg 6}, 3, \mathbf{\neg 2} \}$ is a tautology because regardless
of value of variable 2 the clause is satisfied. Therefore $C$
can be removed.

Figure \ref{fig:hte} plots the total timeout if hidden tautology elimination is
performed only on instances that contain at least some number of
literals. HTE is useful even for very small instances.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{data/hte.pdf}
  \caption{Performance versus minimum instance size for Hidden Tautology Elimination}
  \label{fig:hte}
\end{figure}


\section{Pure Literal Rule (PL)}

Pure literal rule was first described in \cite{Davis:1960:CPQ:321033.321034}
as follows:

\begin{mydef}[Pure Literal Rule]
  If a variable $u$ occurs in formula $F$ in CNF only positively, or
  if $u$ occurs only negatively, then all clauses which contain $u$
  may be deleted. The resulting formula $F'$ is insonsistent
  if and only if $F$ is. (If $F'$ is empty, then if is consistent).
  The polarity that appears in $F$ is called \emph{pure literal}.
\end{mydef}

\begin{myprop}
  If $u$ is a pure literal in $F$ then $F \equiv F \cup \{ u \}$.
\end{myprop}


\section{Splitting Instances}

Take for example formula $F$ in \ref{eqn:split-example} which is a conjunction of
two independent formula $F_1$ and $F_2$. $F$ is consistent only if
both $F_1$ and $F_2$ are consistent. If $F_1$ or $F_2$ are inconsistent
then $F$ is inconsistent.

\begin{align}
  F &= (1 \lor 2 \lor 3) \land (1 \lor \neg 4) \land (5 \lor \neg 6) \land (6 \lor \neg 7) \\
  \label{eqn:split-example}
  F_1 &= (1 \lor 2 \lor 3) \land (1 \lor \neg 4) \\
  F_2 &= (5 \lor \neg 6) \land (6 \lor \neg 7) \\
  F &= F_1 \land F_2
\end{align}

The algorithm \ref{alg:split} implements the splitting algorithm. Splitting formula
provides another opportunity to parallelize.
\todo{Add graph with time versus instance size}
\todo{Histogram of generated instance size}

\begin{myprop}
  If $F(u_1, \ldots, u_{k_1}, \ldots, u_{k_2}, \ldots, u_{k_n})
      = F_1(u_1, \ldots, u_{k_1}) \land \ldots
      \land F_n(u_{k_{n-1}+1}, \ldots, u_{k_n})$
  for $1 < k_1 < \ldots <k_n$ then $F$ is consistent whenever every
  instance $F_1, \ldots, F_n$ is consistent.
\end{myprop}

\begin{algorithm}[h]
  \begin{algorithmic}
    \REQUIRE{$F$ a Boolean formula in CNF}
    \ENSURE{$F_{u_1}, \ldots, F_{u_n} \text{ such that } F = F_{u_1} \land \ldots \land F_{u_n}$}

    \STATE $D \gets$ disjoint sets // \cite{Tarjan:1975:EGB:321879.321884}
    \FOR[for every clause in formula]{$C \in F$}
      \STATE $u \gets$ a variable in clause C
      \FOR[for every variable in clause]{$v \in C$}
        \STATE $D.join(u, v)$
      \ENDFOR
    \ENDFOR

    \FOR[for every clause in formula]{$C \in F$}
      \STATE $u \gets$ a variable in clause C
      \STATE $i \gets D.find(u)$
      \STATE $F_i \gets F_i \cup C$
    \ENDFOR
  \end{algorithmic}

  \caption{Splitting a Boolean formula}
  \label{alg:split}
\end{algorithm}



\section{XOR Gates Extraction}

Many encoded circuits have XOR/XNOR gates which are equation in
$\mathbb{Z}_2$ (figure \ref{tbl:xor-formula}).  The algorithm
to encode a XOR/XNOR gate into a circuit is given in
\cite{Roy_restoringcircuit}.

\begin{table}
  \centering
  \framebox{
    \begin{tabular}{ll}
      \emph{Gate type} & \emph{Boolean formula} \\
      \hline
      \text{XOR} & $0 = u_0 \oplus \ldots \oplus u_{n-1}$ \\
      \text{XNOR} & $1 = u_0 \oplus \ldots \oplus u_{n-1}$ \\
    \end{tabular}
  }

  \caption{XOR/XNOR gates}
  \label{tbl:xor-formula}
\end{table}

Encoding a XOR gate with $n$ inputs in a CNF formula requires
$2^{n-1}$ clauses of length $n$, one literal for each input.
Every clause contains an odd number of negations leading $2^{n-1}$
different combinations. Encoding a XNOR gate is similar, except
that clauses will contain an even number of negations. Example
of encodings are given in table \ref{tbl:xor-encoding}.

Negating an input (i.e. one of the literals) in the X(N)OR gate
is equivalent with reversing the type of gate (XOR $\leftrightarrow$ XNOR).
To specify a gate will use only variables without negations and its type.

Extracting X(N)OR gates helps because they shorten the formula
considerable. 32 clauses of 6 literals each will be replaced by
a single gate of 6 variables.  However, many reasoning algorithms
ignore X(N)OR gates which makes them less powerful. As a trade-off I
expand small games having at most 3 inputs.

\begin{table}
  \centering
  \framebox{
    \begin{tabular}{ll}
      \emph{X(N)OR gate} & \emph{Encoding} \\
      \hline
      $0 = u \oplus v$ & $(\neg u \lor v) \land (u \lor \neg v)$ \\
      $1 = u \oplus v \oplus t$ & $(u \lor v \lor t) \land (\neg u \lor \neg v \lor t) \land
      (\neg u \lor v \lor \neg t) \land (u \lor \neg v \lor \neg t)$ 
    \end{tabular}
  }

  \caption{Examples of encoding a XOR gate with 2 inputs and XNOR gate with
  3 inputs}
  \label{tbl:xor-encoding}
\end{table}

\begin{table}[h]
  \centering
  \framebox{
    \begin{tabular}{l | r r r}
      Test name & in X(N)OR & in CNF & percent \\
      \hline
      AProVE09-24.cnf & 366734 & 760056 & 48,25 \\
      AProVE09-22.cnf & 65416 & 138984 & 47,07 \\
      AProVE09-25.cnf & 205992 & 447524 & 46,03 \\
      AProVE09-13.cnf & 38150 & 94732 & 40,27 \\
      AProVE09-19.cnf & 162024 & 410098 & 39,51 \\
      AProVE09-12.cnf & 145400 & 371302 & 39,16 \\
      AProVE09-05.cnf & 68630 & 177140 & 38,74 \\
      AProVE09-06.cnf & 361904 & 942686 & 38,39 \\
      AProVE09-08.cnf & 39716 & 103498 & 38,37 \\
      AProVE09-07.cnf & 39716 & 103528 & 38,36 \\
      AProVE09-11.cnf & 109548 & 285640 & 38,35 \\
      AProVE09-10.cnf & 312994 & 884978 & 35,37 \\
      AProVE09-17.cnf & 133408 & 383728 & 34,77 \\
      AProVE09-15.cnf & 372164 & 1077424 & 34,54 \\
      AProVE09-20.cnf & 123232 & 385046 & 32,00 \\
      AProVE09-03.cnf & 46820 & 718720 & 6,51 \\
      AProVE09-01.cnf & 72476 & 1127880 & 6,43 \\
      AProVE09-21.cnf & 19180 & 305486 & 6,28 \\
    \end{tabular}
  }
  \caption{Literals in X(N)OR gates encodings versus
  all literals in formula for some well known
  SAT instances (see \ref{app:instances})}
\end{table}
\todo{Add number of DV variables}

\todo{Add citation for algorithm (Cryptominisat?)}
\begin{algorithm}[h]
  \begin{algorithmic}
    \REQUIRE{$F$ a Boolean formula in CNF}
    \ENSURE{extracts the XOR gates}

    \STATE Remove identical clauses in $F$.

    \FOR[for every clause in formula]{$C \in F$}
      \STATE $i \gets$ index of $C$ in $F$
      \STATE $n_{xor}, n_{xnor} \gets 0, 0$

      \FOR{$C_j \in F$ such that $C_j$ has the same variables as $C$}
        \IF{$C_j$ has odd number of negations}
          \STATE $n_{xor} \gets n_{xor} + 1$
        \ELSE
          \STATE $n_{xnor} \gets n_{xnor} + 1$
        \ENDIF
      \ENDFOR

      \STATE $n \gets |C|$
      \IF[Found a XOR gate]{$n_{xor} = 2^{n-1}$}
        \PRINT XOR gate with variables of $C$
        \STATE Remove clauses composing the XOR gate
      \ENDIF

      \IF[Found a XNOR gate]{$n_{xor} = 2^{n-1}$}
        \PRINT XNOR gate with variables of C
        \STATE Remove clauses composing the XNOR gate
      \ENDIF
    \ENDFOR
  \end{algorithmic}

  \caption{Extract X(N)OR gates}
  \label{alg:split}
\end{algorithm}

\todo{From now on formula contain XOR gates}
\todo{Unit propagation versus XOR gates (todo: not implemented yet)}
\todo{Confusing XOR gate: sometimes just XOR, sometimes both XOR/XNOR}

\begin{mydef}[Summing X(N)OR gates]
  If $C_0: a = u_0 \oplus \ldots \oplus u_{n-1}$ and $C_1: b =
  v_0 \oplus \ldots \oplus v_{m-1}$ are two X(N)OR gates let $C_0
  \bigoplus C_1: a \oplus b = u_0 \oplus \ldots \oplus u_{n-1}
  \oplus v_0 \oplus \ldots \oplus v_{m-1}$. \footnote{Note that
  under $\mathbb{Z}_2$, $z \oplus z = 0$, so if a variable appears
  twice in the right side of the equation it can be removed.}
\end{mydef}

\begin{myprop}
  \label{myprop:xor-sum}
  Let $C_0, C_1 \in F$ be two XOR gates. $F \equiv F \setminus \{C_1\} \cup
  \{C_0 \bigoplus C_1\}$.
\end{myprop}

\begin{proof}
\end{proof}

According to the last proposition the X(N)OR gates in a formula $F$ can
be handled as a system of linear equations in $\mathbb{Z}_2$. Multiple X(N)OR
gates can be summed together to knock out variables.

The reverse of \ref{myprop:xor-sum} is also true and in fact is used
to split large XOR gates which otherwise would generate too many
disjunctive clauses. If $a = u_0 \oplus \ldots \oplus u_{n-1}$ where
$a \in \{0, 1\}$, then an extra variable, $z$, called \emph{mutex}, can
be used to split it in two smaller X(N)OR gates 
$a = u_0 \oplus \ldots \oplus u_{k} \oplus z \land 0 = z \oplus u_{k + 1} \oplus
\ldots \oplus u_{n}$.

\section{Dependent Variable Removal (DVR)}
\todo{What graphs?}

\emph{Dependent Variable Removal} is a technique introduced in
\cite{mine:march} which simplifies Boolean formulas containing XOR gates.

\begin{mydef}
  A variable is called \emph{dependent} wrt $F$ if it appears only in XOR gates.
  \footnote{XXX Originally, if it appeared in a single (or two in cryptominisat) XOR gates.}
\end{mydef}

A mutex (recall from previous section that mutexes are used to split
large XOR gates into smaller gates) is a dependent variable since it
always appears in two XOR gates.

\begin{myprop}[DVR]
  \label{myprop:dvr-single}
  Let $u$ be a dependent variable that appears in exactly one XOR gate
  $C \in F$. $F \equiv F \setminus \{C\}$.
\end{myprop}
\todo{Not exactly right since $F$ and $F \setminus C$ have
different number of variables}

\begin{proof}
  Any satisfying assignment of $F$ is a satisfying assignment of $F
  \setminus \{ C \}$.

  Given that $C$ is a XOR gate, the $u$ can be assigned from
  a satisfying assignment of $F \setminus \{C\}$ such that $C$ is
  satisfied $\Rightarrow F$ is satisfied.
\end{proof}


Let $u$ be a dependent variable that appears in XOR gates $C_{0},
..., C_{n-1} \in F$. Let $C'_i = C_i \bigoplus C_0$ for $i = 1,
\ldots, n-1$.  From proposition \ref{myprop:xor-sum}, $F \equiv F'
= F \setminus \{C_{1}, ..., C_{n-1}\} \cup \{C'_{1}, ..., C'_{n-1}\}$.
According to proposition \ref{myprop:dvr-single}, $C_0$ can be removed
from $F'$. Applying this algorithm repeatedly DVR can remove all dependent
variables (including mutexes).


\begin{algorithm}[h]
  \begin{algorithmic}
    \REQUIRE{$F$ a Boolean formula in CNF with XOR gates}
    \ENSURE{$F$ without dependent variables}

    \STATE $S \gets \emptyset$ // $S$ is the set of independent variables
    \FOR[for every clause (not gate)]{$C \in F$}
      \STATE $S \gets S \cup C$
    \ENDFOR

    \FOR[for every variable in $F$]{$u$ variable in $F$}
      \IF[$u$ is dependent]{$u \not \in S$}
        \STATE $C \gets$ smallest XOR gate containing $u$
        \FOR[for every other gate containing $u$]{$C', u \in C'$}
          \STATE $F \gets F \setminus \{C'\} \cup \{C \bigoplus C'\}$
        \ENDFOR
      \ENDIF
    \ENDFOR
  \end{algorithmic}

  \caption{Dependent Variable Removal}
  \label{alg:hyper-resolution}
\end{algorithm}


\section{Blocked Clause Elimination}

Blocked clauses are introduced in
\cite{Kullmann:1999:NMD:312269.312271} and are used to improve
the theortical complixity of the algorithm for solving 3-SAT to
$O(1.5044\ldots^n)$. Blocked clauses are a set of clauses that can
be removed from formula without affecting its satisfiability.

\todo{define resolvent for literal}

\begin{mydef}A clause $C$ is called blocked for literal $l$ with
respect to formula $F$ iff all resolvets of $C$ with $C' \in F$
for $\neg u \in C'$ are tautological, i.e., iff

\[
  v \in C \land \forall C' \in F (\neg u \in C' \Rightarrow \exists \neg v \in C'
  (\neg v \not = \neg u \land v \in C))
\]
\end{mydef}

\section{Variable Elimination}

\section{Hyper Binary Resolution}

\label{sec:hyper-binary-resolution}

Hyper resolution is realized by applying binary resolution repeatedly.

\todo{HBR versus lookahead}

\begin{myprop}[Hyper-unary resolution]
  $(u \rightarrow \neg v_1) \land \ldots
  \land (u \rightarrow \neg v_n) \land (v_1 \lor \dots \lor v_n)
  \Rightarrow \neg u$
\end{myprop}

\begin{myprop}[Hyper-binary resolution]
  $(u \rightarrow \neg v_1) \land \ldots
  \land (u \rightarrow \neg v_n)
  \land (v_1 \lor \dots \lor v_n \lor t) \Rightarrow (\neg u \lor t)$
\end{myprop}

The algorithm \ref{alg:hyper-resolution} applies hyper-\{unary,
binary\} on a CNF formula. In practice, many binary clauses generated
are redundant (e.g. two consecutive runs of hyper-resolution will
generate almost the same clauses).

\begin{algorithm}[h]
  \begin{algorithmic}
    \REQUIRE{$F$ a Boolean formula in CNF}
    \ENSURE{new units, binaries}

    \STATE $\Sigma_0, \Sigma_0' \gets \{\}, 0$
    \STATE $\Sigma_1, \Sigma_1' \gets \{\}, 0$
    \FOR[for every clause in formula]{$C \in F$}
      \FOR[for every literal in clause]{$u \in C$}
        \STATE $\Sigma_0' \gets \Sigma_0' + 1$
        \STATE $\Sigma_1' \gets \Sigma_1' + u$
        \FORALL[for every neighbour of literal]{$v$ s.t. $v \rightarrow \neg u$}
          \STATE $\Sigma_0[v] \gets \Sigma_0[v] + 1$
          \STATE $\Sigma_1[v] \gets \Sigma_1[v] + u$
        \ENDFOR
      \ENDFOR
      \FORALL{literals $u$}
        \IF[Hyper-unary resolution]{$\Sigma_0[u] = \Sigma_0'$}
          \PRINT{new unit $u$}
        \ELSIF[Hyper-binary resolution]{$\Sigma_0[u] = \Sigma_0' - 1$}
          \STATE $v \gets \Sigma_1' - \Sigma_1[u]$
          \PRINT{new binary $u \lor v$}
        \ENDIF
      \ENDFOR
    \ENDFOR
  \end{algorithmic}

  \caption{Hyper resolution}
  \label{alg:hyper-resolution}
\end{algorithm}

Figure \ref{fig:hur} plots the total timeout if hyper-unit resolution
is performed only on instances that contain at least some number
of literals. \todo{Perform similar analysis for HBR}.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{data/hur.pdf}
  \caption{Performance versus minimum instance size for Hyper Unit Resolution}
  \label{fig:hur}
\end{figure}


\section{Subsumming and Self-Subsumming}

\begin{myprop}[Subsumming]
  If a clause includes another clause then the former is satisfied whenever the
  later is satisfied.
  $C_1, C_2 \in F \land C_1 \subseteq C_2 \Rightarrow F \equiv F \setminus \{
  C_2 \}$
\end{myprop}

\begin{myprop}[Self-subsumming]
  $\begin{array}{rl}
    \neg u \lor v_1 \lor \ldots \lor v_i &\lor v_{i+1} \lor \ldots \lor v_n \\
    u \lor v_1 \lor \ldots \lor v_i & \\
    \hline
    v_1 \lor \ldots \lor v_i &\lor v_{i+1} \lor \ldots \lor v_n \\
    u \lor v_1 \lor \ldots \lor v_i &
  \end{array}$

  If a clause is almost included in another clause except one literal whose
  polarity is reversed that literal can be removed from the larger clause.
\end{myprop}

\begin{figure}[h]
  \includegraphics[width=\textwidth]{data/sss.pdf}
  \caption{Performance versus minimum instance size for Self-Subsumming}
  \label{fig:sss}
\end{figure}

Figure \ref{fig:sss} plots the total timeout if self-subsumming
is performed only on instances that contain at least some number
of literals.

\todo{Explain relation between VE and SSS}

